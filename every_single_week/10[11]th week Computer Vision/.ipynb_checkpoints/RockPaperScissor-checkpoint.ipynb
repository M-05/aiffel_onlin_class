{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9279beb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3709ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\r\n",
      "drwxr-xr-x 2 root root 4096 Feb  2 01:52 paper\r\n",
      "drwxr-xr-x 2 root root 4096 Feb  2 01:52 rock\r\n",
      "drwxr-xr-x 2 root root 4096 Feb  2 01:52 scissor\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/aiffel/rock_scissor_paper/scissor\n",
    "!mkdir -p ~/aiffel/rock_scissor_paper/rock\n",
    "!mkdir -p ~/aiffel/rock_scissor_paper/paper\n",
    "\n",
    "!ls -l ~/aiffel/rock_scissor_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43a0ba27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open rock.zip, rock.zip.zip or rock.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "# 원하는 디렉토리로 이동 =3\n",
    "!cd ~/aiffel/rock_scissor_paper/rock\n",
    "\n",
    "# 압축 해제 명령어 : unzip <파일명>.zip\n",
    "!unzip rock.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78d45a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel\n"
     ]
    }
   ],
   "source": [
    "cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4d4118cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/rock_scissor_paper/rock\n"
     ]
    }
   ],
   "source": [
    "cd aiffel/rock_scissor_paper/rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5d5cf238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.jpg   20.jpg  31.jpg  42.jpg  53.jpg  64.jpg  75.jpg  86.jpg  97.jpg\r\n",
      "10.jpg  21.jpg  32.jpg  43.jpg  54.jpg  65.jpg  76.jpg  87.jpg  98.jpg\r\n",
      "11.jpg  22.jpg  33.jpg  44.jpg  55.jpg  66.jpg  77.jpg  88.jpg  99.jpg\r\n",
      "12.jpg  23.jpg  34.jpg  45.jpg  56.jpg  67.jpg  78.jpg  89.jpg  9.jpg\r\n",
      "13.jpg  24.jpg  35.jpg  46.jpg  57.jpg  68.jpg  79.jpg  8.jpg   rock.zip\r\n",
      "14.jpg  25.jpg  36.jpg  47.jpg  58.jpg  69.jpg  7.jpg   90.jpg\r\n",
      "15.jpg  26.jpg  37.jpg  48.jpg  59.jpg  6.jpg   80.jpg  91.jpg\r\n",
      "16.jpg  27.jpg  38.jpg  49.jpg  5.jpg   70.jpg  81.jpg  92.jpg\r\n",
      "17.jpg  28.jpg  39.jpg  4.jpg   60.jpg  71.jpg  82.jpg  93.jpg\r\n",
      "18.jpg  29.jpg  3.jpg   50.jpg  61.jpg  72.jpg  83.jpg  94.jpg\r\n",
      "19.jpg  2.jpg   40.jpg  51.jpg  62.jpg  73.jpg  84.jpg  95.jpg\r\n",
      "1.jpg   30.jpg  41.jpg  52.jpg  63.jpg  74.jpg  85.jpg  96.jpg\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "45c11403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  rock.zip\n",
      "replace 0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip rock.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9cc6d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/rock_scissor_paper/paper\n"
     ]
    }
   ],
   "source": [
    "cd ../paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "596e476a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper.zip\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "af5b4f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  paper.zip\r\n",
      " extracting: 0.jpg                   \r\n",
      " extracting: 1.jpg                   \r\n",
      " extracting: 2.jpg                   \r\n",
      " extracting: 3.jpg                   \r\n",
      " extracting: 4.jpg                   \r\n",
      " extracting: 5.jpg                   \r\n",
      " extracting: 6.jpg                   \r\n",
      " extracting: 7.jpg                   \r\n",
      " extracting: 8.jpg                   \r\n",
      " extracting: 9.jpg                   \r\n",
      " extracting: 10.jpg                  \r\n",
      " extracting: 11.jpg                  \r\n",
      " extracting: 12.jpg                  \r\n",
      " extracting: 13.jpg                  \r\n",
      " extracting: 14.jpg                  \r\n",
      " extracting: 15.jpg                  \r\n",
      " extracting: 16.jpg                  \r\n",
      " extracting: 17.jpg                  \r\n",
      " extracting: 18.jpg                  \r\n",
      " extracting: 19.jpg                  \r\n",
      " extracting: 20.jpg                  \r\n",
      " extracting: 21.jpg                  \r\n",
      " extracting: 22.jpg                  \r\n",
      " extracting: 23.jpg                  \r\n",
      " extracting: 24.jpg                  \r\n",
      " extracting: 25.jpg                  \r\n",
      " extracting: 26.jpg                  \r\n",
      " extracting: 27.jpg                  \r\n",
      " extracting: 28.jpg                  \r\n",
      " extracting: 29.jpg                  \r\n",
      " extracting: 30.jpg                  \r\n",
      " extracting: 31.jpg                  \r\n",
      " extracting: 32.jpg                  \r\n",
      " extracting: 33.jpg                  \r\n",
      " extracting: 34.jpg                  \r\n",
      " extracting: 35.jpg                  \r\n",
      " extracting: 36.jpg                  \r\n",
      " extracting: 37.jpg                  \r\n",
      " extracting: 38.jpg                  \r\n",
      " extracting: 39.jpg                  \r\n",
      " extracting: 40.jpg                  \r\n",
      " extracting: 41.jpg                  \r\n",
      " extracting: 42.jpg                  \r\n",
      " extracting: 43.jpg                  \r\n",
      " extracting: 44.jpg                  \r\n",
      " extracting: 45.jpg                  \r\n",
      " extracting: 46.jpg                  \r\n",
      " extracting: 47.jpg                  \r\n",
      " extracting: 48.jpg                  \r\n",
      " extracting: 49.jpg                  \r\n",
      " extracting: 50.jpg                  \r\n",
      " extracting: 51.jpg                  \r\n",
      " extracting: 52.jpg                  \r\n",
      " extracting: 53.jpg                  \r\n",
      " extracting: 54.jpg                  \r\n",
      " extracting: 55.jpg                  \r\n",
      " extracting: 56.jpg                  \r\n",
      " extracting: 57.jpg                  \r\n",
      " extracting: 58.jpg                  \r\n",
      " extracting: 59.jpg                  \r\n",
      " extracting: 60.jpg                  \r\n",
      " extracting: 61.jpg                  \r\n",
      " extracting: 62.jpg                  \r\n",
      " extracting: 63.jpg                  \r\n",
      " extracting: 64.jpg                  \r\n",
      " extracting: 65.jpg                  \r\n",
      " extracting: 66.jpg                  \r\n",
      " extracting: 67.jpg                  \r\n",
      " extracting: 68.jpg                  \r\n",
      " extracting: 69.jpg                  \r\n",
      " extracting: 70.jpg                  \r\n",
      " extracting: 71.jpg                  \r\n",
      " extracting: 72.jpg                  \r\n",
      " extracting: 73.jpg                  \r\n",
      " extracting: 74.jpg                  \r\n",
      " extracting: 75.jpg                  \r\n",
      " extracting: 76.jpg                  \r\n",
      " extracting: 77.jpg                  \r\n",
      " extracting: 78.jpg                  \r\n",
      " extracting: 79.jpg                  \r\n",
      " extracting: 80.jpg                  \r\n",
      " extracting: 81.jpg                  \r\n",
      " extracting: 82.jpg                  \r\n",
      " extracting: 83.jpg                  \r\n",
      " extracting: 84.jpg                  \r\n",
      " extracting: 85.jpg                  \r\n",
      " extracting: 86.jpg                  \r\n",
      " extracting: 87.jpg                  \r\n",
      " extracting: 88.jpg                  \r\n",
      " extracting: 89.jpg                  \r\n",
      " extracting: 90.jpg                  \r\n",
      " extracting: 91.jpg                  \r\n",
      " extracting: 92.jpg                  \r\n",
      " extracting: 93.jpg                  \r\n",
      " extracting: 94.jpg                  \r\n",
      " extracting: 95.jpg                  \r\n",
      " extracting: 96.jpg                  \r\n",
      " extracting: 97.jpg                  \r\n",
      " extracting: 98.jpg                  \r\n",
      " extracting: 99.jpg                  \r\n",
      " extracting: 100.jpg                 \r\n",
      " extracting: 101.jpg                 \r\n",
      " extracting: 102.jpg                 \r\n",
      " extracting: 103.jpg                 \r\n",
      " extracting: 104.jpg                 \r\n",
      " extracting: 105.jpg                 \r\n",
      " extracting: 106.jpg                 \r\n",
      " extracting: 107.jpg                 \r\n",
      " extracting: 108.jpg                 \r\n",
      " extracting: 109.jpg                 \r\n",
      " extracting: 110.jpg                 \r\n",
      " extracting: 111.jpg                 \r\n",
      " extracting: 112.jpg                 \r\n",
      " extracting: 113.jpg                 \r\n",
      " extracting: 114.jpg                 \r\n",
      " extracting: 115.jpg                 \r\n"
     ]
    }
   ],
   "source": [
    "!unzip paper.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "789037d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/rock_scissor_paper/scissor\n"
     ]
    }
   ],
   "source": [
    "cd ../scissor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ec0e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  scissor.zip\r\n",
      " extracting: 0.jpg                   \r\n",
      " extracting: 1.jpg                   \r\n",
      " extracting: 2.jpg                   \r\n",
      " extracting: 3.jpg                   \r\n",
      " extracting: 4.jpg                   \r\n",
      " extracting: 5.jpg                   \r\n",
      " extracting: 6.jpg                   \r\n",
      " extracting: 7.jpg                   \r\n",
      " extracting: 8.jpg                   \r\n",
      " extracting: 9.jpg                   \r\n",
      " extracting: 10.jpg                  \r\n",
      " extracting: 11.jpg                  \r\n",
      " extracting: 12.jpg                  \r\n",
      " extracting: 13.jpg                  \r\n",
      " extracting: 14.jpg                  \r\n",
      " extracting: 15.jpg                  \r\n",
      " extracting: 16.jpg                  \r\n",
      " extracting: 17.jpg                  \r\n",
      " extracting: 18.jpg                  \r\n",
      " extracting: 19.jpg                  \r\n",
      " extracting: 20.jpg                  \r\n",
      " extracting: 21.jpg                  \r\n",
      " extracting: 22.jpg                  \r\n",
      " extracting: 23.jpg                  \r\n",
      " extracting: 24.jpg                  \r\n",
      " extracting: 25.jpg                  \r\n",
      " extracting: 26.jpg                  \r\n",
      " extracting: 27.jpg                  \r\n",
      " extracting: 28.jpg                  \r\n",
      " extracting: 29.jpg                  \r\n",
      " extracting: 30.jpg                  \r\n",
      " extracting: 31.jpg                  \r\n",
      " extracting: 32.jpg                  \r\n",
      " extracting: 33.jpg                  \r\n",
      " extracting: 34.jpg                  \r\n",
      " extracting: 35.jpg                  \r\n",
      " extracting: 36.jpg                  \r\n",
      " extracting: 37.jpg                  \r\n",
      " extracting: 38.jpg                  \r\n",
      " extracting: 39.jpg                  \r\n",
      " extracting: 40.jpg                  \r\n",
      " extracting: 41.jpg                  \r\n",
      " extracting: 42.jpg                  \r\n",
      " extracting: 43.jpg                  \r\n",
      " extracting: 44.jpg                  \r\n",
      " extracting: 45.jpg                  \r\n",
      " extracting: 46.jpg                  \r\n",
      " extracting: 47.jpg                  \r\n",
      " extracting: 48.jpg                  \r\n",
      " extracting: 49.jpg                  \r\n",
      " extracting: 50.jpg                  \r\n",
      " extracting: 51.jpg                  \r\n",
      " extracting: 52.jpg                  \r\n",
      " extracting: 53.jpg                  \r\n",
      " extracting: 54.jpg                  \r\n",
      " extracting: 55.jpg                  \r\n",
      " extracting: 56.jpg                  \r\n",
      " extracting: 57.jpg                  \r\n",
      " extracting: 58.jpg                  \r\n",
      " extracting: 59.jpg                  \r\n",
      " extracting: 60.jpg                  \r\n",
      " extracting: 61.jpg                  \r\n",
      " extracting: 62.jpg                  \r\n",
      " extracting: 63.jpg                  \r\n",
      " extracting: 64.jpg                  \r\n",
      " extracting: 65.jpg                  \r\n",
      " extracting: 66.jpg                  \r\n",
      " extracting: 67.jpg                  \r\n",
      " extracting: 68.jpg                  \r\n",
      " extracting: 69.jpg                  \r\n",
      " extracting: 70.jpg                  \r\n",
      " extracting: 71.jpg                  \r\n",
      " extracting: 72.jpg                  \r\n",
      " extracting: 73.jpg                  \r\n",
      " extracting: 74.jpg                  \r\n",
      " extracting: 75.jpg                  \r\n",
      " extracting: 76.jpg                  \r\n",
      " extracting: 77.jpg                  \r\n",
      " extracting: 78.jpg                  \r\n",
      " extracting: 79.jpg                  \r\n",
      " extracting: 80.jpg                  \r\n",
      " extracting: 81.jpg                  \r\n",
      " extracting: 82.jpg                  \r\n",
      " extracting: 83.jpg                  \r\n",
      " extracting: 84.jpg                  \r\n",
      " extracting: 85.jpg                  \r\n",
      " extracting: 86.jpg                  \r\n",
      " extracting: 87.jpg                  \r\n",
      " extracting: 88.jpg                  \r\n",
      " extracting: 89.jpg                  \r\n",
      " extracting: 90.jpg                  \r\n",
      " extracting: 91.jpg                  \r\n",
      " extracting: 92.jpg                  \r\n",
      " extracting: 93.jpg                  \r\n",
      " extracting: 94.jpg                  \r\n",
      " extracting: 95.jpg                  \r\n",
      " extracting: 96.jpg                  \r\n",
      " extracting: 97.jpg                  \r\n",
      " extracting: 98.jpg                  \r\n",
      " extracting: 99.jpg                  \r\n",
      " extracting: 100.jpg                 \r\n",
      " extracting: 101.jpg                 \r\n",
      " extracting: 102.jpg                 \r\n",
      " extracting: 103.jpg                 \r\n",
      " extracting: 104.jpg                 \r\n",
      " extracting: 105.jpg                 \r\n",
      " extracting: 106.jpg                 \r\n",
      " extracting: 107.jpg                 \r\n",
      " extracting: 108.jpg                 \r\n",
      " extracting: 109.jpg                 \r\n"
     ]
    }
   ],
   "source": [
    "!unzip scissor.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214e43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc9ad5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8bd42b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/aiffel/aiffel/rock_scissor_paper/rock'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scissor_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "scissor_path\n",
    "paper_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "paper_path\n",
    "rock_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "rock_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d1db619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n"
     ]
    }
   ],
   "source": [
    "scissor = resize_images(scissor_path)\n",
    "paper = resize_images(paper_path)\n",
    "rock = resize_images(rock_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3abdc36",
   "metadata": {},
   "source": [
    "# 가위바위보 이미지 개수 총합에 주의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18a650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300): \n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs= np.zeros(number_of_data * img_size * img_size * color, dtype=np.int32).reshape(number_of_data, \n",
    "                                                                                  img_size,img_size, \n",
    "                                                                                  color)\n",
    "    labels= np.zeros(number_of_data, dtype= np.int32)\n",
    "\n",
    "    idx=0\n",
    "#scissor data\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "#rock data\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'): \n",
    "        img = np.array(Image.open(file), dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "#paper data    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)= load_data(image_dir_path)\n",
    "x_train_norm = x_train / 255.0   \n",
    "# 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7351ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682779 200\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(x_train), np.argmax(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647708d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16237 0\n"
     ]
    }
   ],
   "source": [
    "print(np.argmin(x_train), np.argmin(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7381383c",
   "metadata": {},
   "source": [
    "`plt.imshow`\n",
    "(\n",
    "    __X,\n",
    "    cmap=None__,\n",
    "    norm=None,\n",
    "    aspect=None,\n",
    "    interpolation=None,\n",
    "    alpha=None,\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    origin=None,\n",
    "    extent=None,\n",
    "    *,\n",
    "    filternorm=True,\n",
    "    filterrad=4.0,\n",
    "    resample=None,\n",
    "    url=None,\n",
    "    data=None,\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "Display data as an image, i.e., on a 2D regular raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "16031ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f53f428a4f0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0])\n",
    "plt.imshow(x_train[100])\n",
    "plt.imshow(x_train[299])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82911b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0], y_train[100], y_train[299])\n",
    "# 0 scissor 1 rock 2 paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a6c7c",
   "metadata": {},
   "source": [
    "### 딥러닝 네트워크 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e51c4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36eb08",
   "metadata": {},
   "source": [
    "x_train shape: (300, 28, 28, 3)  \n",
    "y_train shape: (300,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a50f8",
   "metadata": {},
   "source": [
    "#### VGG16 형태로\n",
    "y_train의 값이 0 1 2 - 정수 형태 -> sparse categorical entropy -> softmax or sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9b918",
   "metadata": {},
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(keras.layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(3, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a3d4e",
   "metadata": {},
   "source": [
    "ValueError:   \n",
    "Negative dimension size caused by subtracting 3 from 2 for  \n",
    "'{{node conv2d_5/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true]  \n",
    "(Placeholder, conv2d_5/Conv2D/ReadVariableOp)'   \n",
    "with input shapes: [?,2,2,256], [3,3,256,256].\n",
    "\n",
    "----\n",
    "ValueError:  \n",
    "Negative dimension size caused by subtracting 3 from 2 for  \n",
    "'{{node conv2d_11/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true]  \n",
    "(Placeholder, conv2d_11/Conv2D/ReadVariableOp)'  \n",
    "with input shapes: [?,2,2,64], [3,3,64,64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6867049",
   "metadata": {},
   "source": [
    "input_shape=(1, img_rows, img_cols) -> input_shape=(img_rows, img_cols, 1)  \n",
    "https://stackoverflow.com/questions/41651628/negative-dimension-size-caused-by-subtracting-3-from-1-for-conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da5440d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67432863",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 30,819\n",
      "Trainable params: 30,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2821b8",
   "metadata": {},
   "source": [
    "#### `SparseCategoricalCrossentropy` \n",
    "(\n",
    "    from_logits=False,\n",
    "    ignore_class=None,\n",
    "    reduction=losses_utils.ReductionV2.AUTO,\n",
    "    name='sparse_categorical_crossentropy'\n",
    ")\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy  \n",
    "#### `tf.keras.optimizers.Adam`\n",
    "(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    jit_compile=True,\n",
    "    name='Adam',\n",
    "    **kwargs\n",
    ")\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam#build  \n",
    "#### `tf.keras.metrics.Accuracy` \n",
    "(\n",
    "    name='accuracy', dtype=None\n",
    ")\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640119e",
   "metadata": {},
   "source": [
    "`model.compile`(\n",
    "    __optimizer='rmsprop',\n",
    "    loss=None,\n",
    "    metrics=None__,\n",
    "    loss_weights=None,\n",
    "    weighted_metrics=None,\n",
    "    run_eagerly=None,\n",
    "    steps_per_execution=None,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761a0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function= tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimize= tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "metric= tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dbffb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "             optimizer = \"adam\",\n",
    "             metrics= [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b27f2",
   "metadata": {},
   "source": [
    "TypeError: 'property' object is not iterable  \n",
    "-> loss_funtion 내용물에 () 붙이니까 되네?! 왜지?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ed8c9",
   "metadata": {},
   "source": [
    "`model.fit`(\n",
    "    __x=None,\n",
    "    y=None,\n",
    "    batch_size=None,\n",
    "    epochs=1__,\n",
    "    verbose='auto',\n",
    "    callbacks=None,\n",
    "    validation_split=0.0,\n",
    "    validation_data=None,\n",
    "    shuffle=True,\n",
    "    class_weight=None,\n",
    "    sample_weight=None,\n",
    "    initial_epoch=0,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None,\n",
    "    validation_batch_size=None,\n",
    "    validation_freq=1,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd9e49dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소값: 0.0  최대값: 0.8431372549019608\n"
     ]
    }
   ],
   "source": [
    "print('최소값:',np.min(x_train_norm), ' 최대값:',np.max(x_train_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffc5d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소값 index: 16237  최대값 index: 682779\n"
     ]
    }
   ],
   "source": [
    "print('최소값 index:',np.argmin(x_train_norm), ' 최대값 index:',np.argmax(x_train_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00ed6757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 28, 28, 3), (300,), (300, 28, 28, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_norm.shape, y_train.shape, x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1914577",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_reshaped = x_train_norm.reshape( -1, 28, 28, 3)  # 데이터갯수에 -1을 쓰면 reshape시 자동계산됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7d32c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 28, 28, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5dbd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9f5933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19/19 [==============================] - 3s 7ms/step - loss: 1.0988 - accuracy: 0.3500\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 1.0886 - accuracy: 0.3300\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 1.0754 - accuracy: 0.4667\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 1.0204 - accuracy: 0.6300\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.9312 - accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.7868 - accuracy: 0.7533\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.6729 - accuracy: 0.7767\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5944 - accuracy: 0.7700\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.6007 - accuracy: 0.7767\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7933\n"
     ]
    }
   ],
   "source": [
    "his = model.fit(x_train_norm,\n",
    "                y_train,\n",
    "                batch_size= 16,\n",
    "                epochs= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca168db2",
   "metadata": {},
   "source": [
    "ValueError: in user code:  \n",
    "ValueError: Input 0 of layer sequential_4 is incompatible with the layer:  \n",
    "expected axis -1 of input shape  \n",
    "to have value 1 but received input with shape (None, 28, 28, 3)\n",
    "\n",
    "----\n",
    "input_shape=(28, 28, 1) -> (28, 28, 3)\n",
    "https://datascience.stackexchange.com/questions/85608/valueerror-input-0-of-layer-sequential-is-incompatible-with-the-layer-expected\n",
    "\n",
    "----\n",
    "RuntimeError: You must compile your model before training/testing.  \n",
    "Use `model.compile(optimizer, loss)`.\n",
    "\n",
    "-----\n",
    "ValueError: Shapes (None, 1) and (None, 3) are incompatible\n",
    "\n",
    "x_train_norm.reshape( -1, 28, 28, 1) -> (-1, 28, 28, 3)\n",
    "\n",
    "----\n",
    "TypeError: 'NoneType' object is not callable\n",
    "\n",
    "x_train_norm -> x_train_reshaped\n",
    "\n",
    "---\n",
    "TypeError: 'NoneType' object is not callable\n",
    "\n",
    "x_train_norm.reshape( -1, 28 * 28 * 3)  \n",
    "-- shape -> (300, 2352)  \n",
    "\n",
    "TypeError: 'NoneType' object is not callable  \n",
    "\n",
    "modle.add's units 8 16 16 3 -> 16 32 32 3\n",
    "\n",
    "---\n",
    "ValueError: Shapes (None, 1) and (None, 3) are incompatible\n",
    "\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "             optimizer = \"adam\",\n",
    "             metrics= [\"accuracy\"])\n",
    "loss_function 등등의 값을 일일이 넣어보니 되네..   \n",
    "버전 때문인가?!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c5e29b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 28, 28, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93940bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
